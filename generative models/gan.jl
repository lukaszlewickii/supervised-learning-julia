"""
Generative Adversarial Networks
"""

#importowanie bibliotek
using Pkg; for p in ("Knet","Colors","Images"); haskey(Pkg.installed(),p) || Pkg.add(p); end
using Knet, Colors, Images, Statistics

#zaadowanie zbioru danych mnist
include(Pkg.dir("Knet","data","mnist.jl")) #MNIST data loader functions
global atype = gpu() >= 0 ? KnetArray{Float32} : Array{Float32}

#zdefiniowanie generatora i dyskryminatora
function mlp(w,x;p=0.0,activation=elu,outputactivation=sigm)
    for i=1:2:length(w)
        x = w[i]*dropout(mat(x),p) .+ w[i+1] # mat() used for flattening images to a vector.
        i<length(w)-1 && (x = activation.(x)) 
    end
    return outputactivation.(x) #output layer
end

"""
global const =Float32(1e-8): Definiuje sta globaln  o wartoci 1e-8 typu Float32.  jest u偶ywane w funkcjach straty jako maa warto zapobiegajca bdowi dzielenia przez zero.
D(w,x;p=0.0) = mlp(w,x;p=p): Definiuje funkcj D, kt贸ra reprezentuje dyskryminator. Przyjmuje trzy argumenty - wagi w, dane wejciowe x oraz opcjonalny parametr p (prawdopodobiestwo dropoutu). Funkcja ta korzysta z funkcji mlp do utworzenia modelu.
G(w,z;p=0.0) = mlp(w,z;p=p): Definiuje funkcj G, kt贸ra reprezentuje generator. Podobnie jak funkcja D, przyjmuje trzy argumenty - wagi w, wektor szum贸w z oraz opcjonalny parametr p. Wykorzystuje funkcj mlp do utworzenia modelu generatora
"""
global const =Float32(1e-8)
D(w,x;p=0.0) = mlp(w,x;p=p)
G(w,z;p=0.0) = mlp(w,z;p=p)

#funkcje straty dla dyskryminatora i generatora 
d(d,x,Gz) = -mean(log.(D(d,x) .+ )+log.((1+) .- D(d,Gz)))/2   
g(g, d, z) = -mean(log.(D(d,G(g,z)) .+ ))           
(input, batch) = atype(randn(Float32, input, batch))  #SampleNoise

#funkcja grad z Knet do obliczenia gradient贸w funkcji straty dla dyskryminatora i generatora.
d  = grad(d) # Discriminator gradient
g  = grad(g) # Generator gradient

#inicjalizacja wag modelu dla warstw ukrytych i warstwy wyjciowej
function initweights(hidden,input, output)
     = Any[];
    x = input
    for h in [hidden... output]
        push!(, atype(xavier(h,x)), atype(zeros(h, 1))) #FC Layers weights and bias
        x = h
    end
    return   #return model params
end

#ta funkcja generuje i zapisuje okrelon liczb obraz贸w przy u偶yciu modelu generatora
function generate_and_save(,number,;fldr="generations/")
    Gz = G([1], ([:ginp], number)) .> 0.5
    Gz = permutedims(reshape(Gz,(28,28,number)), (2,1,3))
    [save(fldr*string(i)*".png",Gray.(Gz[:,:,i])) for i=1:number]
end

#trening modelu
function runmodel(, data, ; dtst=nothing, optim=nothing, train=false, saveinterval=20)
    gloss = dloss = total=0.0;
    B = [:batchsize]
    for i=1:(train ? [:epochs] : 1)
        for (x,_) in data
            total+=2B
            Gz = G([1], ([:ginp], B)) #Generate Fake Images
            train ? update!([2], d([2],x,Gz), optim[2]) : (dloss += 2B*d([2], x, Gz))
            
            z=([:ginp],2B) #Sample z from Noise
            train ? update!([1], g([1], [2], z), optim[1]) : (gloss += 2B*g([1],[2],z))       
        end
        train ? runmodel(, dtst, ; train=false) : println((gloss/total, dloss/total))
        i % saveinterval == 0 && generate_and_save(, 100, )  # save 10 images
    end
end

function main()
    =Dict(:batchsize=>32,:epochs=>80,:ginp=>256,:genh=>[512],:disch=>[512],:optim=>Adam,:lr=>0.0002);
    xtrn,ytrn,xtst,ytst = mnist()
    global dtrn = minibatch(xtrn, ytrn, [:batchsize]; xtype=atype)
    global dtst = minibatch(xtst, ytst, [:batchsize]; xtype=atype)
      = (g,d)  = initweights([:genh], [:ginp], 784), initweights([:disch], 784, 1)
      = (pg,pd) = optimizers(g, [:optim]; lr=[:lr]), optimizers(d,[:optim]; lr=[:lr])
    generate_and_show(,100,)
    runmodel(, dtst, ; optim=, train=false) # initial losses
    runmodel(, dtrn, ; optim=, train=true, dtst=dtst) # training 
    ,,,(dtrn,dtst)    # return weights,optimizers,options and dataset
end

#wywoanie caego skryptu, inicjalizacja modelu, trening i wyniki
main()
